#!/bin/bash
clear

echo "ğŸš€ Starting Automated ETL Setup!"

# 1ï¸âƒ£ Clone the Project Repository
WORKDIR="/app"

echo "ğŸ”¹ Cloning the project from GitHub..."
rm -rf $WORKDIR/*  # Ensure clean state
git clone https://github.com/yourusername/yourproject.git $WORKDIR || { echo "âŒ Failed to clone repository! Exiting..."; exit 1; }

cd $WORKDIR || { echo "âŒ Failed to enter the project directory! Exiting..."; exit 1; }

echo "âœ… Repository cloned successfully."

# 2ï¸âƒ£ Create & Write Environment Variables Dynamically
echo "ğŸ”¹ Creating .env file..."

rm -f .env  # Remove existing .env

cat > .env <<EOL
AIRFLOW_POSTGRES_USER=airflow
AIRFLOW_POSTGRES_PASSWORD=airflow
AIRFLOW_DB=airflow
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW_ADMIN_EMAIL=admin@example.com
AIRFLOW_DB_HOST=airflow_postgres

TFL_API_URL=https://api.tfl.gov.uk/AccidentStats
START_YEAR=2005
END_YEAR=2019

USE_CLOUD_DB=False
DB_HOST=postgres_db_tfl_accident_data
DB_PORT=5432
DB_NAME=tfl_accidents
DB_USER=admin
DB_PASSWORD=admin

GCS_CSV_PATH=processed_data/raw/csv/
DBT_PROFILES_DIR=/usr/app/dbt
DBT_PROJECT_NAME=tfl_accidents_project

LOCAL_STORAGE=/opt/airflow/processed_data/raw/csv
EOL

echo "âœ… .env file created successfully."

# Create secrets folder for storing credentials
mkdir -p secrets

# 3ï¸âƒ£ Authenticate with Google Cloud
echo "ğŸ”¹ Please log in to your GCP account..."
gcloud auth login || { echo "âŒ GCP login failed! Exiting..."; exit 1; }

GCP_PROJECT_ID=$(gcloud config get-value project)
echo "ğŸ”¹ Using GCP project: $GCP_PROJECT_ID"
echo "GCP_PROJECT_ID=$GCP_PROJECT_ID" >> .env

# 4ï¸âƒ£ Create Service Account for Storage
STORAGE_ADMIN_NAME="storage-admin"
STORAGE_ADMIN_EMAIL="$STORAGE_ADMIN_NAME@$GCP_PROJECT_ID.iam.gserviceaccount.com"
KEY_FILE_PATH="secrets/gcp_credentials.json"

# Check if Service Account exists
EXISTING_SA=$(gcloud iam service-accounts list --format="value(email)" --filter="email:$STORAGE_ADMIN_EMAIL")

if [ -z "$EXISTING_SA" ]; then
    echo "ğŸ”¹ Creating new Storage Admin service account..."
    gcloud iam service-accounts create $STORAGE_ADMIN_NAME --display-name "Storage Admin Service Account"

    # Assign IAM roles
    echo "ğŸ”¹ Assigning IAM roles..."
    for role in "roles/storage.admin" "roles/storage.objectAdmin"; do
        gcloud projects add-iam-policy-binding $GCP_PROJECT_ID \
            --member="serviceAccount:$STORAGE_ADMIN_EMAIL" --role="$role"
    done

    # Generate service account key
    echo "ğŸ”¹ Generating service account key..."
    gcloud iam service-accounts keys create $KEY_FILE_PATH --iam-account=$STORAGE_ADMIN_EMAIL
    chmod 644 $KEY_FILE_PATH
else
    echo "âœ… Service account already exists."
fi

# Store credentials path in .env
echo "GOOGLE_APPLICATION_CREDENTIALS=$KEY_FILE_PATH" >> .env

echo "âœ… GCP authentication complete."

# 5ï¸âƒ£ Deploy Infrastructure (Terraform)
echo "ğŸ”¹ Deploying infrastructure with Terraform..."
cd terraform || { echo "âŒ Terraform folder not found! Exiting..."; exit 1; }

terraform init
terraform apply -var="project_id=$GCP_PROJECT_ID" -auto-approve

# Get the created bucket name
DATALAKE_BUCKET_NAME="${GCP_PROJECT_ID}-datalake"

# Store bucket name in .env
echo "GCS_BUCKET=$DATALAKE_BUCKET_NAME" >> ../.env
cd ..

echo "âœ… Infrastructure setup complete."

# 6ï¸âƒ£ Start Docker & Airflow
echo "ğŸ”¹ Starting Docker services..."
docker-compose up --build -d

# 7ï¸âƒ£ Wait for Airflow
echo "â³ Waiting for Airflow to initialize..."
sleep 20  # Adjust if needed

# 8ï¸âƒ£ Trigger DAG
DAG_ID="end_to_end_pipeline"

echo "ğŸ”¹ Unpausing and triggering DAG: $DAG_ID..."
docker exec -it airflow-webserver airflow dags unpause $DAG_ID
docker exec -it airflow-webserver airflow dags trigger $DAG_ID

# 9ï¸âƒ£ Monitor DAG Status
echo "ğŸ” Checking DAG status..."

MAX_RETRIES=50  # Increase retries because DAG takes time
RETRY_COUNT=0

while [[ $RETRY_COUNT -lt $MAX_RETRIES ]]; do
    STATUS=$(docker exec -it airflow-webserver airflow dags list-runs -d $DAG_ID --limit 1 --output json | jq -r '.[0].state')

    case "$STATUS" in
        "running")
            echo "â³ DAG is still running... Checking again in 20 seconds."
            sleep 20
            ;;
        "success")
            echo "âœ… DAG completed successfully!"
            break
            ;;
        "failed")
            echo "âŒ DAG failed! Check Airflow logs for more details."
            exit 1
            ;;
        *)
            echo "âš ï¸ Unknown DAG status: $STATUS"
            exit 1
            ;;
    esac

    ((RETRY_COUNT++))
done

if [[ $RETRY_COUNT -eq $MAX_RETRIES ]]; then
    echo "âš ï¸ DAG did not complete within the expected time. Please check Airflow manually."
fi

# ğŸ”Ÿ Wait for Streamlit Dashboard
echo "ğŸ”¹ Waiting for the Streamlit Dashboard to start..."
DASHBOARD_PORT=8501
RETRIES=20

for i in $(seq 1 $RETRIES); do
    if curl --silent --fail "http://localhost:$DASHBOARD_PORT" > /dev/null; then
        echo "âœ… Streamlit Dashboard is now available!"
        break
    else
        echo "â³ Streamlit Dashboard is still starting... Retrying in 10 seconds."
        sleep 10
    fi
done

# ğŸ”Ÿ Final Message
echo "âœ… Setup Complete!"
echo "ğŸ“Š Streamlit Dashboard is now available at: http://localhost:$DASHBOARD_PORT"
